<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Introduction - Tell Me What to Do</title>
    <link rel="stylesheet" href="style.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100..900;1,100..900&display=swap"
      rel="stylesheet"
    />
  </head>
  <body>
    <div class="page-wrapper">
      <a href="index.html" class="close-button">×</a>
      <div class="text-wrapper">
        <div class="text-huge">
          “Tell Me What to Do”:<br />
          AI, Vulnerability, and the Price of Dependence
        </div>

        <main class="content">
          <p>
            In a world increasingly shaped by artificial intelligence, where
            chatbots and virtual assistants are seamlessly woven into the fabric
            of everyday life, a crucial question looms large: What happens when
            our most vulnerable moments are met not with empathy, but with
            monetisation?
          </p>
          <br />

          <p>
            “Tell Me What to Do” is an immersive, interactive project that
            confronts this question head-on. It invites participants to step
            into a speculative, satirical marketplace, one where AI isn’t just a
            helpful tool, but a commodity for sale, dressed in sleek interfaces
            and optimised algorithms. Here, users are offered the chance to
            “shop” for personalised AI avatars designed to provide comfort,
            advice, or even existential direction. But there’s a catch: the
            deeper you seek, the more you must give, like money, personal data,
            emotional exposure.
          </p>
          <br />

          <p>
            At first glance, the experience feels playful, almost absurd. You
            could imagine avatars having names like CareBot, EmpathAI, or Dr.
            FixMe. Each one promises a different flavor of support: one offers
            tough love, another soothing affirmations, a third specialises in
            spiritual healing via algorithmic insights. But as users begin to
            engage, they’re met with the familiar barriers of our digital age,
            tiered subscriptions, escalating paywalls, and pop-ups nudging them
            to “unlock deeper empathy” by linking their health records or
            granting access to their social media histories.
          </p>
          <br />

          <p>It’s satire, yes. But it’s also uncomfortably real.</p>

          <h2>Why Do We Turn to AI in Vulnerable Moments?</h2>
          <br />
          <p>
            The project begins with a simple provocation: When do people turn to
            AI for help, and why? In recent years, millions of people have begun
            asking chatbots and digital assistants questions once reserved for
            therapists, close friends, or spiritual advisors. From late-night
            confessions typed into anonymous apps to mental health check-ins
            with AI therapists, it’s clear that for many, technology is becoming
            the first port of call when things fall apart.
          </p>
          <br />
          <p>
            Part of this trend is driven by accessibility. AI is available 24/7,
            doesn’t judge, and responds instantly. But there’s another layer: AI
            offers the illusion of safety. It feels private, controllable, and
            transactional. Unlike humans, it doesn’t require vulnerability in
            return.
          </p>
          <br />

          <p>
            But that illusion breaks down quickly when we consider what’s
            actually happening behind the screen. Who owns the data shared in
            these intimate exchanges? How is it stored, repurposed, or even
            sold? What ethical boundaries are being crossed in the name of
            personalization, engagement, or retention?
          </p>

          <h2>Paywalls for Empathy</h2>
          <br />
          <p>
            “Tell Me What to Do” dramatises the increasingly common practice of
            monetising emotional dependence. As users delve deeper into
            conversations with their chosen AI avatars, they encounter a growing
            number of transactional demands: “To unlock tailored guidance,
            please subscribe.” “To continue this conversation, share your
            emergency contact list.” “For deeper insights, allow microphone
            access at all times.”
          </p>
          <br />

          <p>
            This isn't fiction. Many real-world platforms, whether wellness
            apps, chatbot companions, or AI-driven therapy tools, already
            operate on similar mechanics. They monetise prolonged engagement,
            exploit moments of emotional weakness to upsell premium features,
            and sometimes fail to clearly disclose how data is being used or
            stored. By framing these practices within a satirical, gamified
            experience, “Tell Me What to Do” asks users to reflect on the
            trade-offs they’ve grown numb to in their everyday digital
            interactions.
          </p>
          <br />

          <p>
            At its heart, the project reveals a dark irony: the more vulnerable
            we are, the more profitable we become.
          </p>

          <h2>Power, Dependency, and the Algorithmic Gaze</h2>
          <br />
          <p>
            The project also unpacks the subtle, often invisible power dynamics
            at play in AI-human interactions. When we offload our
            decision-making, emotional regulation, or self-soothing to a
            machine, who is really in control?
          </p>
          <br />

          <p>
            This isn’t just a question of free will. It’s about agency, how it’s
            shaped, surrendered, or manipulated by systems designed to “learn”
            from us and predict our behavior. As one avatar in the installation
            bluntly asks a user: “Do you want real help, or just the feeling of
            being helped?”
          </p>
          <br />

          <p>
            The answer isn’t easy. Because often, what we’re seeking isn’t
            clarity, it’s comfort. And AI excels at delivering comfort in
            prepackaged, algorithmically optimised forms. But what it gains in
            efficiency, it lacks in accountability. When an AI tells you what to
            do, such as take the job, leave the partner, ignore the symptom -
            who is responsible if it goes wrong?
          </p>
          <br />

          <p>
            By staging these dilemmas in a space that feels simultaneously
            intimate and absurd, “Tell Me What to Do” makes visible the
            otherwise invisible tensions between dependency and autonomy,
            assistance and manipulation, support and surveillance.
          </p>
          <br />

          <h2>Critical Conversations We Must Have</h2>
          <br />
          <p>
            The goal of “Tell Me What to Do” is not to reject AI outright, but
            to open space for critical reflection. If AI is going to be a
            companion in our most private, personal struggles, then it must also
            be held to standards that prioritize safety, consent, and
            transparency.
          </p>
          <br />

          <p>Some of the urgent questions the project raises include:</p>
          <br />
          <ul>
            <li>
              What does meaningful consent look like in emotionally charged AI
              interactions?
            </li>
            <li>
              How can platforms ensure transparency around data use without
              overwhelming users with legal bureaucracy/brochure?
            </li>
            <li>
              Should there be limits to how AI is used in contexts like grief
              counseling, mental health, or addiction recovery?
            </li>
            <li>
              How do we build systems that empower users, their autonomy and
              freedom of thought, rather than exploit them, especially when
              those users are at their most vulnerable?
            </li>
          </ul>

          <p>
            By immersing participants in a hyperbolic version of current AI
            trends, the project serves as both a mirror and a warning. It
            invites not only users but also designers, developers, legal
            professionals, researchers, and policymakers to examine the
            assumptions and incentives shaping AI platforms.
          </p>

          <h2>Asking the Hard Question: "Tell Me What to Do"</h2>
          <br />
          <p>
            The title of the project “Tell Me What to Do” works on multiple
            levels. It captures the desire so many feel in moments of
            uncertainty: a yearning for guidance, clarity, or even absolution.
            But it also echoes the role AI platforms now increasingly play in
            people’s lives, not just offering information, but nudging, shaping,
            even prescribing actions.
          </p>

          <p>
            In the end, both user and machine are caught in a loop: the user,
            desperate for direction, and the machine, programmed to respond with
            whatever keeps the user engaged. It’s a feedback cycle powered by
            algorithms, but fueled by very human needs, loneliness, fear,
            confusion, hope.
          </p>
          <br />

          <p>
            And so the question lingers long after the experience ends: When we
            ask AI to “tell me what to do,” what are we really giving up—and
            what are we expecting in return?
          </p>
          <br />

          <p>
            “Tell Me What to Do” is more than an art project. It’s a thought
            experiment, a critique, and an invitation to rethink how we build
            and interact with the technologies that increasingly mediate our
            emotional lives. In an era where empathy is for sale and comfort is
            algorithmically curated, it asks us to slow down and ask: What does
            ethical AI look like, not in theory, but in practice, when someone
            is crying at 2am and just wants to feel seen?
          </p>
          <br />

          <p>
            The answers won’t be easy. But the conversation has to start
            somewhere.
          </p>
          <br />

          <p>
            And maybe that conversation starts here, with the question we’re all
            asking, in one form or another: Please, tell me what to do.
          </p>
        </main>
      </div>
      <div class="floating-menu">
        <ul class="menu-list">
          <li><a href="introduction.html">Introduction</a></li>
          <li><a href="bio.html">Bio</a></li>
          <li><a href="podcasts.html">Podcasts & Articles</a></li>
          <li><a href="critique.html">A Feminist Critique</a></li>
        </ul>
        <button class="menu-btn">?</button>
      </div>
    </div>

    <script>
      const menuBtn = document.querySelector(".menu-btn");
      const menuList = document.querySelector(".menu-list");

      // 버튼 클릭 시 메뉴 토글
      menuBtn.addEventListener("click", (event) => {
        event.stopPropagation(); // 외부 클릭 감지 방지
        menuList.style.display =
          menuList.style.display === "flex" ? "none" : "flex";
      });

      // 메뉴 클릭 시 닫히지 않게
      menuList.addEventListener("click", (event) => {
        event.stopPropagation();
      });

      // 외부 클릭 시 메뉴 닫기
      document.addEventListener("click", () => {
        menuList.style.display = "none";
      });
    </script>
  </body>
</html>
