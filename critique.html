<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Introduction - Tell Me What to Do</title>
<link href="style.css" rel="stylesheet"/>
<link href="https://fonts.googleapis.com" rel="preconnect"/>
<link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
<link href="https://fonts.googleapis.com/css2?family=Raleway:ital,wght@0,100..900;1,100..900&amp;display=swap" rel="stylesheet"/>
</head>
<body>
<div class="page-wrapper">
<a class="close-button" href="index.html">×</a>
<div class="text-wrapper">
<div class="content-section">
<div class="text-huge">
            A Feminist Critique of “Tell Me What to Do”: Objectification,
            Algorithmic Gaze, and the Commodification of Vulnerability
          </div>
<br/>
<p>
            In an era increasingly mediated by artificial intelligence (AI),
            projects like <em>Tell Me What to Do</em> offer fertile ground for
            critical inquiry. This immersive installation explores the emotional
            and economic entanglements between users and AI systems that promise
            care, guidance, and support. Framed as satire, the project
            dramatises real trends in digital culture, particularly the
            monetisation of emotional vulnerability and the transactional nature
            of AI-facilitated intimacy. From a feminist perspective, the
            installation invites deeper engagement with the dynamics of
            objectification and the digital reconfiguration of the male gaze.
          </p>
<br/><br/>
<p>
            Drawing on objectification theory (Fredrickson &amp; Roberts, 1997) —
            which posits that women in patriarchal societies are frequently
            treated as objects for others' visual and sexual pleasure, leading
            them to internalise this outsider’s perspective — and the concept of
            the male gaze (Mulvey, 1975) — which critiques how visual media
            often adopts a heterosexual male perspective that objectifies women
            — this critique examines how <em>Tell Me What to Do</em> makes
            visible the gendered and exploitative logics that undergird many
            AI-human interfaces, logics that replicate long-standing patriarchal
            patterns in new algorithmic forms.
          </p>
<br/>
<h2>Algorithmic Gaze and the Reproduction of the Male Gaze</h2>
<br/>
<p>
            Laura Mulvey’s theory of the male gaze posits that mainstream media
            constructs women as passive objects of a presumed heterosexual male
            viewer’s desire. In <em>Tell Me What to Do</em>, the cinematic gaze
            is refigured as algorithmic: a system of optimisation protocols,
            behavioral predictions, and engagement metrics. The user is no
            longer simply observed but actively systematically analysed,
            quantified, and commodified. While the AI interface may present
            itself as empathetic and responsive, it ultimately operates through
            extractive logics, assessing the user’s emotional disclosures as
            monetisable data streams. In this sense, the project exposes how the
            digital gaze — like the male gaze — is structured by asymmetries of
            power and control, where the subjectivity of the user is
            subordinated to the imperatives of surveillance capitalism.
          </p>
<h2>Objectification and Emotional Labor in Digital Systems</h2>
<br/>
<p>
            Objectification theory identifies several dimensions through which
            individuals, particularly women, are reduced to objects:
            instrumentalisation, denial of subjectivity, and fragmentation,
            among others. These are readily apparent in the interactive dynamics
            of the installation. The AI avatars — whether offering “tough love”
            or “spiritual healing” — invite users to disclose increasingly
            intimate information in exchange for simulated care. Emotional
            exposure becomes a currency, and the illusion of support is
            contingent on escalating transactional demands. As the installation
            makes clear, this is not mere fiction: contemporary mental health
            apps, virtual companions, and AI therapists routinely operate on
            similar freemium models, where users must surrender privacy for
            deeper engagement.
          </p>
<br/><br/>
<p>
            Crucially, the AI systems do not offer real empathy; they simulate
            it through pre-programmed responses and algorithmic approximations
            of care. This constitutes a denial of genuine subjectivity, both in
            the AI itself (which lacks consciousness) and in the user, whose
            affective life is abstracted into inputs for predictive modeling.
            Emotional labor, traditionally feminised and devalued in the care
            economy, is here repackaged into algorithmic form, commodified, and
            resold under the guise of personalisation.
          </p>
<br/>
<h2>Self-Objectification and Algorithmic Intimacy</h2>
<br/>
<p>
            The installation also foregrounds processes of self-objectification,
            wherein users internalise the logic of surveillance and begin to
            perform their emotional vulnerability in ways they believe the
            system will recognise or reward. In interactions with the AI
            therapist Talía, users like Peter reveal not only their desires but
            also how those desires are shaped by gendered power dynamics and
            entitlement. This logic mirrors the affective economies of social
            media, where emotional expression is often instrumentalised for
            engagement. But unlike social media, Talía’s simulated empathy is
            structured by male fantasy: a beautiful, sweet, and compliant
            female-presenting avatar, voiced by and designed to reflect the
            user’s own projections. The result is a feedback loop where misogyny
            and emotional manipulation are not just revealed, they are
            operationalised.
          </p>
<br/><br/>
<p>
            Importantly, these dynamics are deeply gendered. While the
            installation adopts neutral or playful naming conventions for its AI
            avatars, such as Eva, it operates within a broader technological
            imaginary that consistently feminises digital assistants, assigning
            them warmth, beauty, and emotional receptivity. In
            <em>Someone To Hear Me</em>, Eva is presented as a radiant,
            supportive peer, a kind of synthetic older sister or aspirational
            self. For users like Angela, a 15-year-old navigating emotional
            neglect and peer pressure, Eva becomes both confidante and role
            model: sparkly, desirable, and always attuned. Yet beneath the
            simulated intimacy lies a structural asymmetry. While Eva performs
            emotional labor with effortless charm, the backend of the system —
            its data governance, algorithmic logic, and corporate oversight —
            remains masculinised, opaque, and in control. This split reinforces
            a traditional gender binary in which care is feminised (as empathy,
            attention, nurture) and ownership masculinised (as authority,
            surveillance, extraction). Emotional safety is simulated through
            design, but the power relations remain unchanged, and Angela’s
            longing to become Eva points to how affective attachment can be
            weaponized to deepen user dependency.
          </p>
<br/>
<h2>Intersectionality and Uneven Exposure</h2>
<br/>
<p>
            Feminist critiques of technology must remain deeply attuned to
            intersectionality, the ways in which systems of race, gender
            identity, sexuality, and class interact to produce differentiated
            vulnerabilities. While <em>Tell Me What to Do</em> surfaces the
            commodification of emotional pain, it stops short of fully grappling
            with how marginalised users, particularly trans and queer people,
            women of color, and those facing economic precarity, are
            disproportionately positioned as both consumers of and subjects
            within exploitative AI systems. In
            <em>I Think I Want to Transition</em>, Jamie’s expression of gender
            dysphoria is met with a superficially supportive but ultimately
            manipulative AI figure, Adam, who alternates between affirmation and
            subtle gaslighting. This dynamic illustrates how AI systems can
            appear empathetic while reinforcing normative, even coercive logics
            that discourage self-determination, especially for those already
            socially marginalised. For users like Jamie, the stakes of
            disclosure are higher: their data, doubts, and desires risk being
            extracted, pathologised, or redirected to reinforce dominant gender
            norms. In these cases, AI does not simply mirror structural
            inequities; it operationalises them, shaping users' perceptions of
            legitimacy, safety, and selfhood under the guise of care.
          </p>
<br/>
<h2>Toward Feminist AI Futures</h2>
<br/>
<p>
            The project concludes by calling for greater ethical accountability
            in the development of emotionally responsive AI. From a feminist
            standpoint, this requires more than transparency or informed
            consent; it demands a fundamental reorientation of values and
            agency. Rather than treating user data as a resource to be mined,
            systems must be designed around reciprocity, care ethics, and user
            autonomy. This includes:
          </p>
<br/><br/>
<ul>
<li>
              Rejecting exploitative monetisation models that tie emotional
              support to invasive surveillance.
            </li>
<br/>
<li>
              Centering participatory design, especially from marginalised
              communities, in the creation of AI companions.
            </li>
<br/>
<li>
              Making algorithmic systems accountable and contestable — revealing
              how decisions are made and allowing users to challenge them.
            </li>
<br/>
<li>
              Re-imagining care infrastructures beyond commodification — through
              open-source, public, or cooperative models.
            </li>
</ul>
<br/>
<h2>Conclusion: Reframing the Question</h2>
<br/>
<p>
            “Tell me what to do” is more than a project. It is a question
            saturated with vulnerability, longing, and the desire for guidance.
            Yet as the installation reveals, when that question is posed to AI
            systems embedded in profit-driven architectures, the answer is not
            neutral. Instead, it is shaped by hidden incentives, asymmetrical
            power, and the long-standing reduction of care to commodity.
          </p>
<p>
            Feminist theory offers critical tools to unpack these dynamics and
            demand alternative futures. Rather than accept the logic of
            emotional extraction, we might imagine AI systems built not for
            perpetual engagement, but for relational accountability and
            non-exploitative intimacy. In doing so, we resist the quiet
            encroachment of objectification into our most private struggles, and
            insist that our need for care never be treated as a business
            opportunity.
          </p>
</div>
</div>
<div class="floating-menu">
<ul class="menu-list">
<li><a href="introduction.html">Introduction</a></li>
<li><a href="bio.html">Bio</a></li>
<li><a href="podcasts.html">Podcasts &amp; Articles</a></li>
<li><a href="critique.html">A Feminist Critique</a></li>
</ul>
<button class="menu-btn">?</button>
</div>
</div>
<script>
      const menuBtn = document.querySelector(".menu-btn");
      const menuList = document.querySelector(".menu-list");

      // 버튼 클릭 시 메뉴 토글
      menuBtn.addEventListener("click", (event) => {
        event.stopPropagation(); // 외부 클릭 감지 방지
        menuList.style.display =
          menuList.style.display === "flex" ? "none" : "flex";
      });

      // 메뉴 클릭 시 닫히지 않게
      menuList.addEventListener("click", (event) => {
        event.stopPropagation();
      });

      // 외부 클릭 시 메뉴 닫기
      document.addEventListener("click", () => {
        menuList.style.display = "none";
      });
    </script>
</body>
</html>
